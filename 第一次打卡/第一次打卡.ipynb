{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.线性回归的原理\n",
    "2.线性回归损失函数、代价函数、目标函数\n",
    "3.优化方法(梯度下降法、牛顿法、拟牛顿法等)\n",
    "4.线性回归的评估指标\n",
    "5.sklearn参数详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、线性回归的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归的一般形式：\n",
    "有数据集$\\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$,其中,$x_i = (x_{i1};x_{i2};x_{i3};...;x_{id}),y_i\\in R$<br> \n",
    "其中n表示变量的数量，d表示每个变量的维度。  \n",
    "可以用以下函数来描述y和x之间的关系："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "f(x) \n",
    "&= \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_dx_d  \\\\\n",
    "&= \\sum_{i=0}^{d}\\theta_ix_i \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何来确定$\\theta$的值，使得$f(x)$尽可能接近y的值呢？均方误差是回归中常用的性能度量，即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta)=\\frac{1}{2}\\sum_{j=1}^{n}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以选择$\\theta$,试图让均方误差最小化;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、线性回归损失函数、代价函数、目标函数\n",
    "* 损失函数(Loss Function)：度量单样本预测的错误程度，损失函数值越小，模型就越好。\n",
    "* 代价函数(Cost Function)：度量全部样本集的平均误差。\n",
    "* 目标函数(Object Function)：代价函数和正则化函数，最终要优化的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用的损失函数包括：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数等；常用的代价函数包括均方误差、均方根误差、平均绝对误差等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td bgcolor=#87CEEB>思考题：既然代价函数已经可以度量样本集的平均误差，为什么还要设定目标函数？</td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、线性回归的优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定初始参数$\\theta$,不断迭代，使得$J(\\theta)$最小化：\n",
    "$$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial{J(\\theta)}}{\\partial\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial\\theta} \n",
    "&= \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2}\\sum_{i=1}^{n}(f_\\theta(x)^{(i)}-y^{(i)})^2 \\\\\n",
    "&= 2*\\frac{1}{2}\\sum_{i=1}^{n}(f_\\theta(x)^{(i)}-y^{(i)})*\\frac{\\partial}{\\partial\\theta_j}(f_\\theta(x)^{(i)}-y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{n}(f_\\theta(x)^{(i)}-y^{(i)})*\\frac{\\partial}{\\partial\\theta_j}(\\sum_{j=0}^{d}\\theta_jx_j^{(i)}-y^{(i)}))\\\\\n",
    "&= \\sum_{i=1}^{n}(f_\\theta(x)^{(i)}-y^{(i)})x_j^{(i)} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_j = \\theta_j + \\alpha\\sum_{i=1}^{n}(y^{(i)}-f_\\theta(x)^{(i)})x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、最小二乘法矩阵求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、拟牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、线性回归的评价指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE, RMSE, MAE, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但以上评价指标都无法消除量纲不一致而导致的误差值差别大的问题，最常用的指标是$R^2$,可以避免量纲不一致问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R^2: = 1-\\frac{\\sum^{m}_{i=1}(y^{(i)} - \\hat y^{(i)})^2}{\\sum^{m}_{i=1}(\\bar y - \\hat y^{(i)})^2} =1-\\frac{\\frac{1}{m}\\sum^{m}_{i=1}(y^{(i)} - \\hat y^{(i)})^2}{\\frac{1}{m}\\sum^{m}_{i=1}(\\bar y - \\hat y^{(i)})^2} = 1-\\frac{MSE}{VAR}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、sklearn.linear_model参数详解："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_intercept : 默认为True,是否计算该模型的截距。如果使用中心化的数据，可以考虑设置为False,不考虑截距。注意这里是考虑，一般还是要考虑截距\n",
    "\n",
    "normalize: 默认为false. 当fit_intercept设置为false的时候，这个参数会被自动忽略。如果为True,回归器会标准化输入参数：减去平均值，并且除以相应的二范数。当然啦，在这里还是建议将标准化的工作放在训练模型之前。通过设置sklearn.preprocessing.StandardScaler来实现，而在此处设置为false\n",
    "\n",
    "copy_X : 默认为True, 否则X会被改写\n",
    "\n",
    "n_jobs: int 默认为1. 当-1时默认使用全部CPUs ??(这个参数有待尝试)\n",
    "\n",
    "可用属性：\n",
    "\n",
    "coef_:训练后的输入端模型系数，如果label有两个，即y值有两列。那么是一个2D的array\n",
    "\n",
    "intercept_: 截距\n",
    "\n",
    "可用的methods:\n",
    "\n",
    "fit(X,y,sample_weight=None):\n",
    "X: array, 稀疏矩阵 [n_samples,n_features]\n",
    "y: array [n_samples, n_targets]\n",
    "sample_weight: 权重 array [n_samples]\n",
    "在版本0.17后添加了sample_weight\n",
    "\n",
    "get_params(deep=True)： 返回对regressor 的设置值\n",
    "\n",
    "predict(X): 预测 基于 R^2值\n",
    "\n",
    "score： 评估\n",
    "\n",
    "参考https://blog.csdn.net/weixin_39175124/article/details/79465558"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align =\"left\";background-color=\"#87CEEB\">\n",
    "<tr>\n",
    "    <td bgcolor=\"#87CEEB\"><font size=2>练习题：请用以下数据（可自行生成尝试，或用其他已有数据集）</font></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td  bgcolor=\"#87CEEB\"><font size=2>1、首先尝试调用sklearn的线性回归函数进行训练；</font></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td bgcolor=\"#87CEEB\"><font size=2>2、用最小二乘法的矩阵求解法训练数据；</font></td>\n",
    "</tr>\n",
    "<tr>    \n",
    "<td  bgcolor=\"#87CEEB\"><font size=2>3、用梯度下降法训练数据；</font></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td  bgcolor=\"#87CEEB\"><font size=2>4、比较各方法得出的结果是否一致。</font></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成数据\n",
    "import numpy as np\n",
    "#生成随机数\n",
    "np.random.seed(1234)\n",
    "x = np.random.rand(500,3)\n",
    "#构建映射关系，模拟真实的数据待预测值,映射关系为y = 4.2 + 5.7*x1 + 10.8*x2，可自行设置值进行尝试\n",
    "y = x.dot(np.array([4.2,5.7,10.8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、先尝试调用sklearn的线性回归模型训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计的参数值为：[ 4.2  5.7 10.8]\n",
      "R2:1.0\n",
      "预测值为: [85.2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 调用模型\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "# 训练模型\n",
    "lr.fit(x,y)\n",
    "print(\"估计的参数值为：%s\" %(lr.coef_))\n",
    "# 计算R平方\n",
    "print('R2:%s' %(lr.score(x,y)))\n",
    "# 任意设定变量，预测目标值\n",
    "x_test = np.array([2,4,5]).reshape(1,-1)\n",
    "y_hat = lr.predict(x_test)\n",
    "print(\"预测值为: %s\" %(y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、最小二乘法的矩阵求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计的参数值：[ 4.2  5.7 10.8]\n",
      "预测值为: [85.2]\n"
     ]
    }
   ],
   "source": [
    "class LR_LS():\n",
    "    def __init__(self):\n",
    "        self.w = None      \n",
    "    def fit(self, X, y):\n",
    "        # 最小二乘法矩阵求解\n",
    "        self.w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    def predict(self, X):\n",
    "        # 用已经拟合的参数值预测新自变量\n",
    "        y_pred = X.dot(self.w)\n",
    "        return y_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lr_ls = LR_LS()\n",
    "    lr_ls.fit(x,y)\n",
    "    print(\"估计的参数值：%s\" %(lr_ls.w))\n",
    "    x_test = np.array([2,4,5]).reshape(1,-1)\n",
    "    print(\"预测值为: %s\" %(lr_ls.predict(x_test)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计的参数值为：[ 4.20000001  5.70000003 10.79999997]\n",
      "预测值为：[85.19999995]\n"
     ]
    }
   ],
   "source": [
    "class LR_GD():\n",
    "    def __init__(self):\n",
    "        self.w = None     \n",
    "    def fit(self,X,y,alpha=0.02,loss = 1e-10): # 设定步长为0.002,判断是否收敛的条件为1e-10\n",
    "        y = y.reshape(-1,1) #重塑y值的维度以便矩阵运算\n",
    "        [m,d] = np.shape(X) #自变量的维度\n",
    "        self.w = np.zeros((d)) #将参数的初始值定为0\n",
    "        tol = 1e5\n",
    "        #============================= show me your code =======================\n",
    "        while tol > loss:\n",
    "            h_f = X.dot(self.w).reshape(-1,1) \n",
    "            theta = self.w + alpha*np.mean(X*(y - h_f),axis=0) #计算迭代的参数值\n",
    "            tol = np.sum(np.abs(theta - self.w))\n",
    "            self.w = theta\n",
    "        #============================= show me your code =======================\n",
    "    def predict(self, X):\n",
    "        # 用已经拟合的参数值预测新自变量\n",
    "        y_pred = X.dot(self.w)\n",
    "        return y_pred  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lr_gd = LR_GD()\n",
    "    lr_gd.fit(x,y)\n",
    "    print(\"估计的参数值为：%s\" %(lr_gd.w))\n",
    "    x_test = np.array([2,4,5]).reshape(1,-1)\n",
    "    print(\"预测值为：%s\" %(lr_gd.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计的参数值为：[5.23630999 6.56800993 8.76618343]\n",
      "预测值为：[80.57557684]\n"
     ]
    }
   ],
   "source": [
    "class LR_GD_M():\n",
    "    def __init__(self):\n",
    "        self.w = None  # 参数\n",
    "        self.k = 0  # m的下标\n",
    "    def fit(self, X, y, alpha=0.02, loss=1e-10):\n",
    "        y = y.reshape(-1, 1)  # 变成列向量\n",
    "        [m, d] = np.shape(X)\n",
    "        self.w = np.zeros((d))\n",
    "        tol = 1e5\n",
    "        \n",
    "        while tol > loss:\n",
    "            if self.k < len(y):\n",
    "                # 抽10个\n",
    "                # 抽取小批量的数据\n",
    "                train_size = x.shape[0]\n",
    "                batch_size = 10 # 抽10个\n",
    "                batch_mask = np.random.choice(train_size, batch_size) # 从6000个数据中随机抽取10个 获得其索引\n",
    "\n",
    "                x_batch = x[batch_mask] # 通过索引取出该值\n",
    "                y_batch = y[batch_mask] # 通过索引去除该监督值\n",
    "                h_f = x_batch.dot(self.w).reshape(-1, 1)\n",
    "                theta = self.w + alpha * np.sum(x_batch * (y_batch - h_f), axis=0)\n",
    "                #h_f = X[self.k:self.k+100,:].dot(self.w).reshape(-1, 1)\n",
    "                #theta = self.w + alpha * np.sum(X[self.k:self.k+100] * (y[self.k:self.k+100] - h_f), axis=0)\n",
    "                tol = np.sum(np.abs(theta - self.w))\n",
    "                self.w = theta\n",
    "                self.k += 10\n",
    "            else:\n",
    "                break\n",
    "    def predict(self, X):\n",
    "        # 用已经拟合的参数值预测新自变量\n",
    "        y_pred = X.dot(self.w)\n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    lr_gd_m = LR_GD_M()\n",
    "    lr_gd_m.fit(x,y)\n",
    "    print(\"估计的参数值为：%s\" %(lr_gd_m.w))\n",
    "    x_test = np.array([2,4,5]).reshape(1,-1)\n",
    "    print(\"预测值为：%s\" %(lr_gd_m.predict(x_test)))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
